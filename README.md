# ExecuTorch Examples & Benchmarking Toolkit

This repository provides practical examples and tools for exporting, running, and benchmarking various deep learning models (LLMs, Vision, etc.) using **ExecuTorch** on both local PCs (WSL/Linux) and Android devices.

---

## ğŸ“‚ Project Structure

Each directory focuses on specific ExecuTorch features or model execution scenarios.

### 1. Running LLMs
Export and run the latest lightweight language models with a chat interface.
- **[Running_Llama](./Running_Llama)**: Running Llama-3.2-1B-Instruct on PC and Android. Includes CPU vs. Vulkan performance comparison.
- **[Running_SmolLM2](./Running_SmolLM2)**: SmolLM2-135M-Instruct example. Includes ChatML format handling and EOS token configuration.
- **[Running_Llava](./Running_Llava)**: Contains PTE files related to LLaVA (Vision-Language Model).

### 2. Core Workflows
Examples for learning the basic model conversion and processing flow of ExecuTorch.
- **[Getting_Started_with_ExecuTorch](./Getting_Started_with_ExecuTorch)**: A minimal "Hello World" example from model export to Python runtime execution.
- **[Model_Export_and_Lowering](./Model_Export_and_Lowering)**: Detailed steps of `torch.export`, Backend Partitioning, Lowering, and external constant (PTD) management.
- **[Building_from_Source](./Building_from_Source)**: How to build C++ apps by including ExecuTorch source directly using `add_subdirectory()`. Includes CPU/GPU benchmarking tools.

### 3. Profiling & Optimization
Measure model performance and analyze graph structures.
- **[Profiling](./Profiling)**: Measure layer-by-layer performance of models like MobileNetV2 using `ETDump` and `ETRecord`.
- **[Graph_Partitioning](./Graph_Partitioning)**: Analyze how model operations are delegated to various backends (XNNPACK, Vulkan, Exynos) and check accelerator assignment.

---

## ğŸ› ï¸ Environment Setup

The examples in this project assume the following environment:

- **OS**: Linux (Ubuntu/WSL2)
- **Python**: Conda environment (e.g., `basic`) recommended
- **ExecuTorch Source**: `/home/tm0118/Desktop/executorch` (Local source reference)
- **Build System**: CMake 3.29+, Ninja
- **Android Support**: Android NDK (r27d recommended), ADB (Android Debug Bridge)

---

## ğŸš€ Quick Start Guide (Llama-3.2 Example)

1. **Export Model**
   ```bash
   cd Running_Llama
   python export_llama.py
   ```
2. **Build C++ Runner**
   ```bash
   cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE=Release
   cmake --build build -j
   ```
3. **Run (PC)**
   ```bash
   ./build/llama_chat --model_path llama3_2_instruct_bf16.pte --tokenizer_path <path_to_tokenizer> --prompt "Hello!"
   ```

---

## ğŸ‘¤ Author
**Taemin Kim**
M.S. Student at Mobile Embedded Systems Lab, Korea University.

---

> **Note**: Build artifacts like `.pte`, `.pth`, and `build/` directories, as well as large model files, are not tracked by Git (see `.gitignore`). They must be generated locally.

<br><br>

---
---

# ExecuTorch ì˜ˆì œ ë° ë²¤ì¹˜ë§ˆí‚¹ íˆ´í‚· (Korean)

ì´ ì €ì¥ì†ŒëŠ” **ExecuTorch**ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸(LLM, Vision ë“±)ì„ ë‚´ë³´ë‚´ê³ (Export), ë¡œì»¬ PC(WSL/Linux) ë° ì•ˆë“œë¡œì´ë“œ ê¸°ê¸°ì—ì„œ ì‹¤í–‰ ë° ë²¤ì¹˜ë§ˆí‚¹í•˜ëŠ” ì‹¤ì „ ì˜ˆì œë“¤ì„ ëª¨ì•„ë†“ì€ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

ê° í´ë”ëŠ” ExecuTorchì˜ íŠ¹ì • ê¸°ëŠ¥ì´ë‚˜ ëª¨ë¸ ì‹¤í–‰ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.

### 1. LLM ì‹¤í–‰ (Running LLMs)
ìµœì‹  ê²½ëŸ‰ ì–¸ì–´ ëª¨ë¸ì„ ExecuTorchë¡œ ë³€í™˜í•˜ê³  ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
- **[Running_Llama](./Running_Llama)**: Llama-3.2-1B-Instruct ëª¨ë¸ì„ PC ë° ì•ˆë“œë¡œì´ë“œì—ì„œ ì‹¤í–‰. (CPU vs Vulkan ì†ë„ ë¹„êµ í¬í•¨)
- **[Running_SmolLM2](./Running_SmolLM2)**: SmolLM2-135M-Instruct ëª¨ë¸ ì‹¤í–‰ ì˜ˆì œ. ChatML í¬ë§· ì²˜ë¦¬ ë° EOS í† í° ì„¤ì • í¬í•¨.
- **[Running_Llava](./Running_Llava)**: LLaVA (Vision-Language Model) ê´€ë ¨ PTE íŒŒì¼ í¬í•¨.

### 2. í•µì‹¬ ì›Œí¬í”Œë¡œìš° (Core Workflows)
ExecuTorchì˜ ê¸°ë³¸ì ì¸ ëª¨ë¸ ë³€í™˜ ë° ì²˜ë¦¬ ê³¼ì •ì„ ìµíˆê¸° ìœ„í•œ ì˜ˆì œì…ë‹ˆë‹¤.
- **[Getting_Started_with_ExecuTorch](./Getting_Started_with_ExecuTorch)**: ëª¨ë¸ Exportë¶€í„° Python ëŸ°íƒ€ì„ ì‹¤í–‰ê¹Œì§€ì˜ ìµœì†Œ ë‹¨ìœ„ "Hello World" ì˜ˆì œ.
- **[Model_Export_and_Lowering](./Model_Export_and_Lowering)**: `torch.export`, Backend Partitioning, Lowering ê³¼ì •ì˜ ìƒì„¸ ë‹¨ê³„ ë° ì™¸ë¶€ ìƒìˆ˜(PTD) ê´€ë¦¬ ì˜ˆì œ.
- **[Building_from_Source](./Building_from_Source)**: `add_subdirectory()` ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ExecuTorch ì†ŒìŠ¤ë¥¼ í”„ë¡œì íŠ¸ì— ì§ì ‘ í¬í•¨ì‹œì¼œ C++ ì•±ì„ ë¹Œë“œí•˜ëŠ” ë°©ë²•. (CPU/GPU ì„±ëŠ¥ ë¹„êµ íˆ´ í¬í•¨)

### 3. ë¶„ì„ ë° ìµœì í™” (Profiling & Optimization)
ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
- **[Profiling](./Profiling)**: `ETDump`, `ETRecord`ë¥¼ í™œìš©í•˜ì—¬ MobileNetV2 ë“±ì˜ ëª¨ë¸ì„ ë ˆì´ì–´ ë‹¨ìœ„(Layer-by-layer)ë¡œ ì„±ëŠ¥ ì¸¡ì •.
- **[Graph_Partitioning](./Graph_Partitioning)**: XNNPACK, Vulkan, Exynos ë“± ë‹¤ì–‘í•œ ë°±ì—”ë“œë¡œ ëª¨ë¸ ì—°ì‚°ì´ ì–´ë–»ê²Œ ë¶„ì‚°(Delegation)ë˜ëŠ”ì§€ ë¶„ì„í•˜ê³  ê°€ì†ê¸° í• ë‹¹ í˜„í™© í™•ì¸.

---

## ğŸ› ï¸ ì£¼ìš” í™˜ê²½ ì„¤ì •

ì´ í”„ë¡œì íŠ¸ì˜ ì˜ˆì œë“¤ì€ ê³µí†µì ìœ¼ë¡œ ì•„ë˜ í™˜ê²½ì„ ì „ì œë¡œ í•©ë‹ˆë‹¤.

- **OS**: Linux (Ubuntu/WSL2)
- **Python**: Conda í™˜ê²½ (`basic` ë“±) ê¶Œì¥
- **ExecuTorch Source**: `/home/tm0118/Desktop/executorch` (ë¡œì»¬ ì†ŒìŠ¤ ì°¸ì¡°)
- **Build System**: CMake 3.29 ì´ìƒ, Ninja
- **Android Support**: Android NDK (r27d ê¶Œì¥), ADB (Android Debug Bridge)

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ (Llama-3.2 ì˜ˆì‹œ)

1. **ëª¨ë¸ Export**
   ```bash
   cd Running_Llama
   python export_llama.py
   ```
2. **C++ ëŸ¬ë„ˆ ë¹Œë“œ**
   ```bash
   cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE=Release
   cmake --build build -j
   ```
3. **ì‹¤í–‰ (PC)**
   ```bash
   ./build/llama_chat --model_path llama3_2_instruct_bf16.pte --tokenizer_path <path_to_tokenizer> --prompt "Hello!"
   ```

---

## ğŸ‘¤ Author
**Taemin Kim**
M.S. Student at Mobile Embedded Systems Lab, Korea University.

---

> **Note**: `.pte`, `.pth`, `build/` í´ë” ë“± ë¹Œë“œ ê²°ê³¼ë¬¼ ë° ëŒ€ìš©ëŸ‰ ëª¨ë¸ íŒŒì¼ì€ `.gitignore`ì— ì˜í•´ ê´€ë¦¬ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¡œì»¬ì—ì„œ ì§ì ‘ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
