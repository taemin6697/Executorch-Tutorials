Order,Node Name,ATen Operator,Backend Assignment,Is Delegated?
1,aten_convolution_default,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
2,aten__native_batch_norm_legit_no_training_default,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
3,getitem,<built-in function getitem>,XNNPACK (Accelerator),YES
4,aten_relu_default,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
5,aten_max_pool2d_with_indices_default,"<EdgeOpOverload: aten.max_pool2d_with_indices.default>: schema = aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)",ExecuTorch (Portable CPU),NO
6,getitem_1,<built-in function getitem>,ExecuTorch (Portable CPU),NO
7,aten_convolution_default_1,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
8,aten__native_batch_norm_legit_no_training_default_1,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
9,getitem_2,<built-in function getitem>,XNNPACK (Accelerator),YES
10,aten_relu_default_1,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
11,aten_convolution_default_2,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
12,aten__native_batch_norm_legit_no_training_default_2,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
13,getitem_3,<built-in function getitem>,XNNPACK (Accelerator),YES
14,aten_relu_default_2,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
15,aten_convolution_default_3,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
16,aten__native_batch_norm_legit_no_training_default_3,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
17,getitem_4,<built-in function getitem>,XNNPACK (Accelerator),YES
18,aten_convolution_default_4,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
19,aten__native_batch_norm_legit_no_training_default_4,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
20,getitem_5,<built-in function getitem>,XNNPACK (Accelerator),YES
21,aten_add_tensor,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
22,aten_relu_default_3,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
23,aten_convolution_default_5,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
24,aten__native_batch_norm_legit_no_training_default_5,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
25,getitem_6,<built-in function getitem>,XNNPACK (Accelerator),YES
26,aten_relu_default_4,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
27,aten_convolution_default_6,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
28,aten__native_batch_norm_legit_no_training_default_6,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
29,getitem_7,<built-in function getitem>,XNNPACK (Accelerator),YES
30,aten_relu_default_5,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
31,aten_convolution_default_7,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
32,aten__native_batch_norm_legit_no_training_default_7,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
33,getitem_8,<built-in function getitem>,XNNPACK (Accelerator),YES
34,aten_add_tensor_1,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
35,aten_relu_default_6,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
36,aten_convolution_default_8,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
37,aten__native_batch_norm_legit_no_training_default_8,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
38,getitem_9,<built-in function getitem>,XNNPACK (Accelerator),YES
39,aten_relu_default_7,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
40,aten_convolution_default_9,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
41,aten__native_batch_norm_legit_no_training_default_9,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
42,getitem_10,<built-in function getitem>,XNNPACK (Accelerator),YES
43,aten_relu_default_8,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
44,aten_convolution_default_10,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
45,aten__native_batch_norm_legit_no_training_default_10,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
46,getitem_11,<built-in function getitem>,XNNPACK (Accelerator),YES
47,aten_add_tensor_2,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
48,aten_relu_default_9,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
49,aten_convolution_default_11,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
50,aten__native_batch_norm_legit_no_training_default_11,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
51,getitem_12,<built-in function getitem>,XNNPACK (Accelerator),YES
52,aten_relu_default_10,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
53,aten_convolution_default_12,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
54,aten__native_batch_norm_legit_no_training_default_12,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
55,getitem_13,<built-in function getitem>,XNNPACK (Accelerator),YES
56,aten_relu_default_11,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
57,aten_convolution_default_13,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
58,aten__native_batch_norm_legit_no_training_default_13,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
59,getitem_14,<built-in function getitem>,XNNPACK (Accelerator),YES
60,aten_convolution_default_14,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
61,aten__native_batch_norm_legit_no_training_default_14,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
62,getitem_15,<built-in function getitem>,XNNPACK (Accelerator),YES
63,aten_add_tensor_3,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
64,aten_relu_default_12,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
65,aten_convolution_default_15,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
66,aten__native_batch_norm_legit_no_training_default_15,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
67,getitem_16,<built-in function getitem>,XNNPACK (Accelerator),YES
68,aten_relu_default_13,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
69,aten_convolution_default_16,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
70,aten__native_batch_norm_legit_no_training_default_16,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
71,getitem_17,<built-in function getitem>,XNNPACK (Accelerator),YES
72,aten_relu_default_14,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
73,aten_convolution_default_17,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
74,aten__native_batch_norm_legit_no_training_default_17,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
75,getitem_18,<built-in function getitem>,XNNPACK (Accelerator),YES
76,aten_add_tensor_4,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
77,aten_relu_default_15,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
78,aten_convolution_default_18,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
79,aten__native_batch_norm_legit_no_training_default_18,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
80,getitem_19,<built-in function getitem>,XNNPACK (Accelerator),YES
81,aten_relu_default_16,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
82,aten_convolution_default_19,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
83,aten__native_batch_norm_legit_no_training_default_19,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
84,getitem_20,<built-in function getitem>,XNNPACK (Accelerator),YES
85,aten_relu_default_17,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
86,aten_convolution_default_20,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
87,aten__native_batch_norm_legit_no_training_default_20,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
88,getitem_21,<built-in function getitem>,XNNPACK (Accelerator),YES
89,aten_add_tensor_5,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
90,aten_relu_default_18,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
91,aten_convolution_default_21,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
92,aten__native_batch_norm_legit_no_training_default_21,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
93,getitem_22,<built-in function getitem>,XNNPACK (Accelerator),YES
94,aten_relu_default_19,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
95,aten_convolution_default_22,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
96,aten__native_batch_norm_legit_no_training_default_22,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
97,getitem_23,<built-in function getitem>,XNNPACK (Accelerator),YES
98,aten_relu_default_20,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
99,aten_convolution_default_23,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
100,aten__native_batch_norm_legit_no_training_default_23,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
101,getitem_24,<built-in function getitem>,XNNPACK (Accelerator),YES
102,aten_add_tensor_6,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
103,aten_relu_default_21,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
104,aten_convolution_default_24,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
105,aten__native_batch_norm_legit_no_training_default_24,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
106,getitem_25,<built-in function getitem>,XNNPACK (Accelerator),YES
107,aten_relu_default_22,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
108,aten_convolution_default_25,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
109,aten__native_batch_norm_legit_no_training_default_25,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
110,getitem_26,<built-in function getitem>,XNNPACK (Accelerator),YES
111,aten_relu_default_23,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
112,aten_convolution_default_26,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
113,aten__native_batch_norm_legit_no_training_default_26,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
114,getitem_27,<built-in function getitem>,XNNPACK (Accelerator),YES
115,aten_convolution_default_27,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
116,aten__native_batch_norm_legit_no_training_default_27,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
117,getitem_28,<built-in function getitem>,XNNPACK (Accelerator),YES
118,aten_add_tensor_7,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
119,aten_relu_default_24,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
120,aten_convolution_default_28,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
121,aten__native_batch_norm_legit_no_training_default_28,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
122,getitem_29,<built-in function getitem>,XNNPACK (Accelerator),YES
123,aten_relu_default_25,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
124,aten_convolution_default_29,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
125,aten__native_batch_norm_legit_no_training_default_29,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
126,getitem_30,<built-in function getitem>,XNNPACK (Accelerator),YES
127,aten_relu_default_26,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
128,aten_convolution_default_30,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
129,aten__native_batch_norm_legit_no_training_default_30,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
130,getitem_31,<built-in function getitem>,XNNPACK (Accelerator),YES
131,aten_add_tensor_8,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
132,aten_relu_default_27,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
133,aten_convolution_default_31,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
134,aten__native_batch_norm_legit_no_training_default_31,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
135,getitem_32,<built-in function getitem>,XNNPACK (Accelerator),YES
136,aten_relu_default_28,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
137,aten_convolution_default_32,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
138,aten__native_batch_norm_legit_no_training_default_32,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
139,getitem_33,<built-in function getitem>,XNNPACK (Accelerator),YES
140,aten_relu_default_29,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
141,aten_convolution_default_33,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
142,aten__native_batch_norm_legit_no_training_default_33,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
143,getitem_34,<built-in function getitem>,XNNPACK (Accelerator),YES
144,aten_add_tensor_9,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
145,aten_relu_default_30,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
146,aten_convolution_default_34,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
147,aten__native_batch_norm_legit_no_training_default_34,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
148,getitem_35,<built-in function getitem>,XNNPACK (Accelerator),YES
149,aten_relu_default_31,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
150,aten_convolution_default_35,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
151,aten__native_batch_norm_legit_no_training_default_35,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
152,getitem_36,<built-in function getitem>,XNNPACK (Accelerator),YES
153,aten_relu_default_32,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
154,aten_convolution_default_36,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
155,aten__native_batch_norm_legit_no_training_default_36,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
156,getitem_37,<built-in function getitem>,XNNPACK (Accelerator),YES
157,aten_add_tensor_10,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
158,aten_relu_default_33,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
159,aten_convolution_default_37,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
160,aten__native_batch_norm_legit_no_training_default_37,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
161,getitem_38,<built-in function getitem>,XNNPACK (Accelerator),YES
162,aten_relu_default_34,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
163,aten_convolution_default_38,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
164,aten__native_batch_norm_legit_no_training_default_38,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
165,getitem_39,<built-in function getitem>,XNNPACK (Accelerator),YES
166,aten_relu_default_35,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
167,aten_convolution_default_39,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
168,aten__native_batch_norm_legit_no_training_default_39,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
169,getitem_40,<built-in function getitem>,XNNPACK (Accelerator),YES
170,aten_add_tensor_11,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
171,aten_relu_default_36,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
172,aten_convolution_default_40,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
173,aten__native_batch_norm_legit_no_training_default_40,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
174,getitem_41,<built-in function getitem>,XNNPACK (Accelerator),YES
175,aten_relu_default_37,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
176,aten_convolution_default_41,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
177,aten__native_batch_norm_legit_no_training_default_41,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
178,getitem_42,<built-in function getitem>,XNNPACK (Accelerator),YES
179,aten_relu_default_38,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
180,aten_convolution_default_42,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
181,aten__native_batch_norm_legit_no_training_default_42,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
182,getitem_43,<built-in function getitem>,XNNPACK (Accelerator),YES
183,aten_add_tensor_12,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
184,aten_relu_default_39,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
185,aten_convolution_default_43,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
186,aten__native_batch_norm_legit_no_training_default_43,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
187,getitem_44,<built-in function getitem>,XNNPACK (Accelerator),YES
188,aten_relu_default_40,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
189,aten_convolution_default_44,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
190,aten__native_batch_norm_legit_no_training_default_44,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
191,getitem_45,<built-in function getitem>,XNNPACK (Accelerator),YES
192,aten_relu_default_41,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
193,aten_convolution_default_45,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
194,aten__native_batch_norm_legit_no_training_default_45,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
195,getitem_46,<built-in function getitem>,XNNPACK (Accelerator),YES
196,aten_convolution_default_46,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
197,aten__native_batch_norm_legit_no_training_default_46,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
198,getitem_47,<built-in function getitem>,XNNPACK (Accelerator),YES
199,aten_add_tensor_13,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
200,aten_relu_default_42,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
201,aten_convolution_default_47,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
202,aten__native_batch_norm_legit_no_training_default_47,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
203,getitem_48,<built-in function getitem>,XNNPACK (Accelerator),YES
204,aten_relu_default_43,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
205,aten_convolution_default_48,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
206,aten__native_batch_norm_legit_no_training_default_48,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
207,getitem_49,<built-in function getitem>,XNNPACK (Accelerator),YES
208,aten_relu_default_44,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
209,aten_convolution_default_49,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
210,aten__native_batch_norm_legit_no_training_default_49,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
211,getitem_50,<built-in function getitem>,XNNPACK (Accelerator),YES
212,aten_add_tensor_14,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
213,aten_relu_default_45,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
214,aten_convolution_default_50,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
215,aten__native_batch_norm_legit_no_training_default_50,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
216,getitem_51,<built-in function getitem>,XNNPACK (Accelerator),YES
217,aten_relu_default_46,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
218,aten_convolution_default_51,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
219,aten__native_batch_norm_legit_no_training_default_51,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
220,getitem_52,<built-in function getitem>,XNNPACK (Accelerator),YES
221,aten_relu_default_47,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
222,aten_convolution_default_52,"<EdgeOpOverload: aten.convolution.default>: schema = aten::convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor",XNNPACK (Accelerator),YES
223,aten__native_batch_norm_legit_no_training_default_52,"<EdgeOpOverload: aten._native_batch_norm_legit_no_training.default>: schema = aten::_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)",XNNPACK (Accelerator),YES
224,getitem_53,<built-in function getitem>,XNNPACK (Accelerator),YES
225,aten_add_tensor_15,"<EdgeOpOverload: aten.add.Tensor>: schema = aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",XNNPACK (Accelerator),YES
226,aten_relu_default_48,<EdgeOpOverload: aten.relu.default>: schema = aten::relu(Tensor self) -> Tensor,XNNPACK (Accelerator),YES
227,aten_mean_dim,"<EdgeOpOverload: aten.mean.dim>: schema = aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor",XNNPACK (Accelerator),YES
228,aten_view_copy_default,"<EdgeOpOverload: aten.view_copy.default>: schema = aten::view_copy(Tensor self, SymInt[] size) -> Tensor",ExecuTorch (Portable CPU),NO
229,aten_permute_copy_default,"<EdgeOpOverload: aten.permute_copy.default>: schema = aten::permute_copy(Tensor self, int[] dims) -> Tensor",XNNPACK (Accelerator),YES
230,aten_addmm_default,"<EdgeOpOverload: aten.addmm.default>: schema = aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor",ExecuTorch (Portable CPU),NO
